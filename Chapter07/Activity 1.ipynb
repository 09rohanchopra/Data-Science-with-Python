{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Concatenate, Reshape, Dropout, BatchNormalization, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../chapter 8/data/movie_reviews.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.SentimentText = data.SentimentText.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \n",
    "    string = re.sub(r\"https?\\://\\S+\", '', string)\n",
    "    string = re.sub(r'\\<a href', ' ', string)\n",
    "    string = re.sub(r'&amp;', '', string) \n",
    "    string = re.sub(r'<br />', ' ', string)\n",
    "    string = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', string)\n",
    "    string = re.sub('\\d','', string)\n",
    "    string = re.sub(r\"can\\'t\", \"cannot\", string)\n",
    "    string = re.sub(r\"it\\'s\", \"it is\", string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.SentimentText = data.SentimentText.apply(lambda x: clean_str(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie    43558\n",
       "film     39095\n",
       "it       30659\n",
       "one      26509\n",
       "is       20355\n",
       "like     20270\n",
       "good     15099\n",
       "the      13913\n",
       "time     12682\n",
       "even     12656\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(' '.join(data['SentimentText']).split()).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') + ['movie', 'film', 'time']\n",
    "stop_words = set(stop_words)\n",
    "remove_stop_words = lambda r: [[word for word in word_tokenize(sente) if word not in stop_words] for sente in sent_tokenize(r)]\n",
    "data['SentimentText'] = data['SentimentText'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "        data['SentimentText'].apply(lambda x: x[0]),\n",
    "        iter=10,\n",
    "        size=16,\n",
    "        window=5,\n",
    "        min_count=5,\n",
    "        workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('movie_embedding.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(text):    \n",
    "    try:\n",
    "        return ' '.join(text[0])\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.SentimentText = data.SentimentText.apply(lambda x: combine_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77348 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(list(data['SentimentText']))\n",
    "sequences = tokenizer.texts_to_sequences(data['SentimentText'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(filename, word_index , num_words, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    file = open(filename, encoding=\"utf-8\")\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coef = np.asarray(values[1:])\n",
    "        embeddings_index[word] = coef\n",
    "    file.close()\n",
    "    \n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for word, pos in word_index.items():\n",
    "        if pos >= num_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[pos] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = load_embedding('movie_embedding.txt', word_index, len(word_index), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews, pd.get_dummies(data.Sentiment), test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((MAX_SEQUENCE_LENGTH,))\n",
    "embedding_layer = Embedding(len(word_index),\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False)(inp)\n",
    "model = Flatten()(embedding_layer)\n",
    "model = BatchNormalization()(model)\n",
    "model = Dropout(0.10)(model)\n",
    "model = Dense(units=256, activation='relu')(model)\n",
    "model = Dense(units=64, activation='relu')(model)\n",
    "model = Dropout(0.5)(model)\n",
    "predictions = Dense(units=2, activation='softmax')(model)\n",
    "model = Model(inputs = inp, outputs = predictions)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 100, 16)           1237568   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1600)              6400      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               409856    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 1,670,402\n",
      "Trainable params: 429,634\n",
      "Non-trainable params: 1,240,768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 18s 898us/step - loss: 0.8252 - acc: 0.5493 - val_loss: 0.6402 - val_acc: 0.6342\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 13s 638us/step - loss: 0.6792 - acc: 0.6069 - val_loss: 0.6102 - val_acc: 0.6720\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 11s 566us/step - loss: 0.6354 - acc: 0.6430 - val_loss: 0.5877 - val_acc: 0.7012\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 12s 609us/step - loss: 0.6143 - acc: 0.6654 - val_loss: 0.5707 - val_acc: 0.7174\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 11s 556us/step - loss: 0.5903 - acc: 0.6874 - val_loss: 0.5533 - val_acc: 0.7298\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 12s 582us/step - loss: 0.5735 - acc: 0.7016 - val_loss: 0.5384 - val_acc: 0.7350\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 12s 605us/step - loss: 0.5557 - acc: 0.7186 - val_loss: 0.5239 - val_acc: 0.7494 loss: 0.5560 - acc: \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 11s 528us/step - loss: 0.5390 - acc: 0.7304 - val_loss: 0.5132 - val_acc: 0.7544\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 12s 587us/step - loss: 0.5256 - acc: 0.7375 - val_loss: 0.5039 - val_acc: 0.7620\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 12s 589us/step - loss: 0.5171 - acc: 0.7485 - val_loss: 0.4959 - val_acc: 0.7634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2101ae46470>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7634"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(preds, 1), np.argmax(y_test.values, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1774</td>\n",
       "      <td>679</td>\n",
       "      <td>2453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>504</td>\n",
       "      <td>2043</td>\n",
       "      <td>2547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>2278</td>\n",
       "      <td>2722</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1   All\n",
       "Actual                     \n",
       "0          1774   679  2453\n",
       "1           504  2043  2547\n",
       "All        2278  2722  5000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual = pd.Series(np.argmax(y_test.values, axis=1), name='Actual')\n",
    "y_pred = pd.Series(np.argmax(preds, axis=1), name='Predicted')\n",
    "pd.crosstab(y_actual, y_pred, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      "love love love another absolutely superb performance miss beginning end one big treat n't rent buy\n",
      "\n",
      "Predicted sentiment = Positive\n",
      "\n",
      "Actual sentiment = Positive\n"
     ]
    }
   ],
   "source": [
    "review_num = 111\n",
    "print(\"Review: \\n\"+tokenizer.sequences_to_texts([X_test[review_num]])[0])\n",
    "sentiment = \"Positive\" if np.argmax(preds[review_num]) else \"Negative\"\n",
    "print(\"\\nPredicted sentiment = \"+ sentiment)\n",
    "sentiment = \"Positive\" if np.argmax(y_test.values[review_num]) else \"Negative\"\n",
    "print(\"\\nActual sentiment = \"+ sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
